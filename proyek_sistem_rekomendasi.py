# -*- coding: utf-8 -*-
"""Proyek Sistem Rekomendasi

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bQSgmpVSobJmD5LeiYNv2HQ4sTEb2eiw

Note: Pendekatan utama yang dipilih adalah Collaborative Filtering, akan tetapi di sini saya akan membuat menggunakan keduanya untuk mencoba dan membandingkan serta memberikan penjelasan dan kesimpulan berdasarkan keduanya di akhir.

# Import Library
"""

# Import Library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse import csr_matrix
from sklearn.neighbors import NearestNeighbors
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import re
import warnings

# TensorFlow dan Keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Suppress all warnings
warnings.filterwarnings('ignore')

"""# Data Loading

Sumber dataset: https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database
"""

# Load dataset
anime = pd.read_csv('anime.csv')

# Menampilkan 5 data teratas
print("5 data teratas dari dataset anime:")
print(anime.head())

# Load dataset
rating = pd.read_csv('rating.csv', on_bad_lines='skip')

# Menampilkan 5 data teratas
print("\n5 data teratas dari dataset rating:")
print(rating.head())

"""# Data Understanding

Tahap Data Understanding bertujuan untuk memperoleh pemahaman awal tentang struktur,
tipe data, dan karakteristik umum dari dataset yang akan digunakan.
Ini adalah langkah krusial sebelum melakukan preprocessing atau pemodelan.
"""

print('Jumlah data anime yang tersedia: ', len(anime.anime_id.unique()))
print('Jumlah data rating anime: ', len(rating.anime_id.unique()))

print("\n--- Informasi Dataset Anime ---")
anime.info()
print("\nStatistik Deskriptif Dataset Anime:")
print(anime.describe())

print("\n--- Informasi Dataset Rating ---")
rating.info()
print("\nStatistik Deskriptif Dataset Rating:")
print(rating.describe())

print('\nJumlah user_id: ', len(rating.user_id.unique()))
print('Jumlah anime_id unik dalam dataset rating: ', len(rating.anime_id.unique()))
print('Jumlah total data rating: ', len(rating))

"""# Univariate Exploratory Data Analysis (EDA)

Univariate EDA berfokus pada analisis dan visualisasi distribusi setiap variabel secara individual.
Tujuannya adalah untuk memahami karakteristik intrinsik dari setiap kolom,
mengidentifikasi pola, outlier, atau masalah data lainnya sebelum pemodelan.

## Distribusi rating di dataset anime (global rating)
"""

plt.figure(figsize=(10, 6))
sns.histplot(anime['rating'], bins=20, kde=True)
plt.title('Distribusi Rating Anime (Global)')
plt.xlabel('Rating')
plt.ylabel('Jumlah Anime')
plt.grid(axis='y', alpha=0.75)
plt.show()

"""Sebagian besar anime memiliki rating di atas 6, menunjukkan distribusi yang condong ke kanan (rating tinggi).

## Distribusi tipe anime
"""

plt.figure(figsize=(10, 6))
sns.countplot(y='type', data=anime, order=anime['type'].value_counts().index)
plt.title('Distribusi Tipe Anime')
plt.xlabel('Jumlah Anime')
plt.ylabel('Tipe')
plt.grid(axis='x', alpha=0.75)
plt.show()

"""Tipe 'TV' mendominasi dataset, diikuti oleh 'OVA'.

## Top 10 Genre Anime Terbanyak
"""

# Untuk EDA genre, kita bisa memisahkan genre dan menghitung frekuensinya
all_genres = anime['genre'].dropna().str.split(', ').explode().reset_index(drop=True) # Reset index here
plt.figure(figsize=(12, 8))
sns.countplot(y=all_genres, order=all_genres.value_counts().head(10).index)
plt.title('Top 10 Genre Anime Terbanyak')
plt.xlabel('Jumlah Anime')
plt.ylabel('Genre')
plt.grid(axis='x', alpha=0.75)
plt.show()

"""Genre 'Comedy' mendominasi dataset

## Distribusi jumlah members (yang menonton/mengikuti anime)
"""

plt.figure(figsize=(10, 6))
sns.histplot(anime['members'], bins=50, kde=True)
plt.title('Distribusi Jumlah Members Anime')
plt.xlabel('Jumlah Members')
plt.ylabel('Jumlah Anime')
plt.grid(axis='y', alpha=0.75)
plt.yscale('log') # Menggunakan skala logaritmik karena distribusi sangat miring
plt.show()

"""Mayoritas anime memiliki jumlah members yang relatif kecil, dengan sedikit anime yang sangat populer (jumlah members tinggi).

## Distribusi rating di dataset rating (user-specific rating)
"""

plt.figure(figsize=(10, 6))
sns.histplot(rating['rating'], bins=10, kde=True)
plt.title('Distribusi Rating User')
plt.xlabel('Rating')
plt.ylabel('Jumlah Rating')
plt.grid(axis='y', alpha=0.75)
plt.show()

"""Ada nilai rating -1 yang perlu ditangani, yang mungkin berarti pengguna menonton tetapi tidak memberikan rating.

Rating positif (1-10) menunjukkan distribusi yang condong ke rating tinggi (6 ke atas).

# Data Preprocessing

Tahap Data Preprocessing melibatkan pembersihan dan transformasi data mentah
menjadi format yang cocok untuk pemodelan. Ini adalah langkah penting untuk
memastikan kualitas data dan kinerja model yang optimal.
"""

print("--- Pemeriksaan Missing Values Awal ---")
print("Missing values di dataset anime:\n", anime.isnull().sum())
print("\nMissing values di dataset rating:\n", rating.isnull().sum())

print("\n--- Pemeriksaan Duplikasi Awal ---")
print("Jumlah data duplikat di dataset anime:", anime.duplicated().sum())
print("Jumlah data duplikat di dataset rating:", rating.duplicated().sum())

"""## Penanganan Dataset anime"""

print("\n--- Proses Cleaning Dataset Anime ---")

# Kolom genre (mengisi missing value dengan "Unknown Genre")
anime['genre'].fillna('Unknown Genre', inplace=True)
print("Missing values di 'genre' setelah diisi:", anime['genre'].isnull().sum())

# Kolom type (mengisi missing value dengan modus atau "Unknown Type")
mode_type = anime['type'].mode()
if not mode_type.empty:
    anime['type'].fillna(mode_type[0], inplace=True)
    print(f"Missing values di 'type' diisi dengan modus: {mode_type[0]}")
else:
    anime['type'].fillna('Unknown Type', inplace=True)
    print("Missing values di 'type' diisi dengan 'Unknown Type' (karena tidak ada modus yang jelas)")
print("Missing values di 'type' setelah diisi:", anime['type'].isnull().sum())

# Kolom rating (rating global anime, mengisi missing value dengan median)
median_rating_anime = anime['rating'].median()
anime['rating'].fillna(median_rating_anime, inplace=True)
print(f"Missing values di 'rating' (anime) diisi dengan median: {median_rating_anime}")
print("Missing values di 'rating' (anime) setelah diisi:", anime['rating'].isnull().sum())

print("\nMissing values di anime setelah cleaning:")
print(anime.isnull().sum())

"""## Penanganan Dataset rating"""

print("\n--- Proses Cleaning Dataset Rating ---")

# Menangani Data Duplikat
print(f"\nJumlah data duplikat di rating sebelum dihapus: {rating.duplicated().sum()}")
rating.drop_duplicates(inplace=True)
print(f"Jumlah data duplikat di rating setelah dihapus: {rating.duplicated().sum()}")

print("Bentuk rating setelah cleaning:", rating.shape)

# Penanganan nilai -1 pada kolom 'rating' di dataset `rating`
# Nilai -1 diyakini sebagai "pengguna menonton tapi tidak memberikan rating".
# Untuk model rekomendasi berbasis rating eksplisit, nilai -1 ini tidak informatif.
# Kita akan menggantinya dengan NaN (atau menghapusnya) dan kemudian mengimputasi atau menghapusnya.
# Pilihan yang lebih baik untuk Collaborative Filtering adalah menghapus rating -1,
# karena rating -1 akan memengaruhi perhitungan kesamaan dan prediksi rating secara negatif.
print(f"\nJumlah rating -1 sebelum dihapus: {len(rating[rating['rating'] == -1])}")
rating = rating[rating['rating'] != -1]
print(f"Jumlah rating -1 setelah dihapus: {len(rating[rating['rating'] == -1])}")
print("Bentuk rating setelah menghapus -1:", rating.shape)

"""## Menggabungkan dataset anime dan rating

menggabungkan anime_id dan nama dari dataset anime ke rating agar rating memiliki informasi nama anime.
"""

merged_data = pd.merge(rating, anime[['anime_id', 'name', 'genre', 'type']], on='anime_id', how='left')

"""Periksa apakah ada missing values yang muncul setelah merge (ini mungkin terjadi jika ada anime_id di rating yang tidak ada di anime.csv)

"""

print("\nMissing values di merged_data setelah merge:")
print(merged_data.isnull().sum())

"""Hapus baris yang mungkin memiliki missing 'name', 'genre', atau 'type' setelah merge"""

merged_data.dropna(subset=['name', 'genre', 'type'], inplace=True)
print("\nBentuk merged_data setelah menghapus baris dengan missing info anime:", merged_data.shape)

"""Ini mengindikasikan bahwa anime_id tersebut tidak ada di dataset 'anime'.

# Data Preparation

Tahap Data Preparation fokus pada transformasi data ke format yang spesifik
untuk setiap jenis model rekomendasi (Content-Based Filtering dan Collaborative Filtering).
Ini termasuk fitur engineering, pembentukan matriks, dan pemisahan data.

## Data Preparation untuk Content-Based Filtering

Content-Based Filtering akan menggunakan informasi 'genre' dan 'type' dari anime.
"""

# Gabungkan kolom 'genre' dan 'type' menjadi satu fitur teks
# Menggunakan dataset `anime` untuk Content-Based Filtering
anime_cbf = anime.copy()
anime_cbf['features'] = anime_cbf['genre'] + ' ' + anime_cbf['type']

# Hapus karakter khusus dari kolom 'name' untuk meningkatkan kualitas data teks
anime_cbf['name'] = anime_cbf['name'].apply(lambda x: re.sub(r'&#\d+;', '', x))

# TF-IDF Vectorization untuk fitur teks
# TfidfVectorizer akan mengkonversi koleksi dokumen teks (fitur anime) menjadi matriks fitur TF-IDF.
tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=3) # min_df=3 untuk mengabaikan kata yang muncul di kurang dari 3 dokumen

# Fit dan transform fitur 'genre' dan 'type'
tfidf_matrix = tfidf_vectorizer.fit_transform(anime_cbf['features'])

print("\nBentuk TF-IDF Matrix (untuk Content-Based Filtering):", tfidf_matrix.shape)
print("Jumlah fitur TF-IDF:", len(tfidf_vectorizer.get_feature_names_out()))

"""## Data Preparation untuk Collaborative Filtering

Collaborative Filtering akan menggunakan dataset `merged_data` (user_id, anime_id, rating)
"""

# Hitung jumlah rating per user
user_counts = merged_data['user_id'].value_counts()
# Filter user yang memiliki minimal N rating (misalnya 50 rating)
min_ratings_per_user = 50
filtered_users = user_counts[user_counts >= min_ratings_per_user].index
merged_data_cf_tf = merged_data[merged_data['user_id'].isin(filtered_users)]

# Hitung jumlah rating per anime
anime_counts = merged_data_cf_tf['anime_id'].value_counts()
# Filter anime yang memiliki minimal M rating (misalnya 50 rating)
min_ratings_per_anime = 50
filtered_anime = anime_counts[anime_counts >= min_ratings_per_anime].index
merged_data_cf_tf = merged_data_cf_tf[merged_data_cf_tf['anime_id'].isin(filtered_anime)]

print(f"\nBentuk dataset untuk Collaborative Filtering (TensorFlow) setelah filtering user dan anime: {merged_data_cf_tf.shape}")
print(f"Jumlah user unik setelah filtering: {merged_data_cf_tf['user_id'].nunique()}")
print(f"Jumlah anime unik setelah filtering: {merged_data_cf_tf['anime_id'].nunique()}")

"""Mengodekan user_id dan anime_id menjadi indeks numerik berurutan"""

user_ids = merged_data_cf_tf['user_id'].unique().tolist()
anime_ids = merged_data_cf_tf['anime_id'].unique().tolist()

user_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}
anime_to_idx = {anime_id: idx for idx, anime_id in enumerate(anime_ids)}

idx_to_user = {idx: user_id for user_id, idx in user_to_idx.items()}
idx_to_anime = {idx: anime_id for anime_id, idx in anime_to_idx.items()}

merged_data_cf_tf['user_encoded'] = merged_data_cf_tf['user_id'].map(user_to_idx)
merged_data_cf_tf['anime_encoded'] = merged_data_cf_tf['anime_id'].map(anime_to_idx)

num_users = len(user_ids)
num_anime = len(anime_ids)

print(f"\nTotal user unik yang dienkode: {num_users}")
print(f"Total anime unik yang dienkode: {num_anime}")

"""Normalisasi rating ke skala 0-1 untuk model"""

min_rating = merged_data_cf_tf['rating'].min()
max_rating = merged_data_cf_tf['rating'].max()
merged_data_cf_tf['rating'] = (merged_data_cf_tf['rating'] - min_rating) / (max_rating - min_rating)
print(f"\nRating dinormalisasi ke rentang [{min_rating:.2f}, {max_rating:.2f}] menjadi [0, 1]")

"""Pisahkan data untuk training dan testing"""

X = merged_data_cf_tf[['user_encoded', 'anime_encoded']].values
y = merged_data_cf_tf['rating'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\nUkuran data training (X_train, y_train): {X_train.shape}, {y_train.shape}")
print(f"Ukuran data testing (X_test, y_test): {X_test.shape}, {y_test.shape}")

"""# Model Development dengan Content-Based Filtering

Content-Based Filtering merekomendasikan item kepada pengguna berdasarkan
atribut-atribut item itu sendiri dan preferensi masa lalu pengguna terhadap atribut tersebut.
Di sini, kita akan menghitung kemiripan antar anime berdasarkan 'genre' dan 'type' mereka.

## Proses Training: Perhitungan Cosine Similarity
"""

print("\n--- Proses Training Content-Based Filtering: Menghitung Cosine Similarity ---")
cosine_sim = cosine_similarity(tfidf_matrix)

print("Bentuk Cosine Similarity Matrix:", cosine_sim.shape)

# Buat DataFrame dari cosine similarity untuk kemudahan indexing
cosine_sim_df = pd.DataFrame(cosine_sim, index=anime_cbf['name'], columns=anime_cbf['name'])

print("\nContoh Cosine Similarity untuk 'Kimi no Na wa.':")
print(cosine_sim_df['Kimi no Na wa.'].sort_values(ascending=False).head(10))

# Fungsi rekomendasi Content-Based Filtering
def content_based_recommendations(anime_name, num_recommendations=10):
    """
    Memberikan rekomendasi anime berdasarkan kemiripan konten.

    Args:
        anime_name (str): Nama anime yang ingin dicari rekomendasinya.
        num_recommendations (int): Jumlah rekomendasi yang diinginkan.

    Returns:
        pd.DataFrame: DataFrame berisi nama anime yang direkomendasikan beserta ratingnya.
    """
    if anime_name not in anime_cbf['name'].values:
        print(f"Anime '{anime_name}' tidak ditemukan dalam dataset.")
        return pd.DataFrame()

    idx = anime_cbf[anime_cbf['name'] == anime_name].index[0]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:num_recommendations+1]
    anime_indices = [i[0] for i in sim_scores]

    recommended_anime = anime_cbf.iloc[anime_indices][['name', 'genre', 'type', 'rating', 'members']]
    return recommended_anime

print("\n--- Demonstrasi Rekomendasi Content-Based Filtering ---")
recommended_anime_cbf = content_based_recommendations('Fullmetal Alchemist: Brotherhood', 10)
print("Rekomendasi untuk 'Fullmetal Alchemist: Brotherhood':")
print(recommended_anime_cbf)

print("\nRekomendasi untuk 'Naruto':")
recommended_anime_naruto = content_based_recommendations('Naruto', 10)
print(recommended_anime_naruto)

"""## Evaluasi Model Content-Based Filtering"""

print("\n--- Evaluasi Model Content-Based Filtering (Kualitatif) ---")
print("""
Evaluasi Content-Based Filtering dilakukan secara kualitatif dengan memeriksa relevansi rekomendasi.
1.  **Rekomendasi untuk 'Fullmetal Alchemist: Brotherhood':**
    Anime seperti 'Fullmetal Alchemist' (original), ' Fullmetal Alchemist: The Sacred Star of Milos  ',
    'Fullmetal Alchemist: Brotherhood Specials' muncul. Ini masuk akal karena mereka seringkali memiliki genre
    'Action', 'Adventure', 'Fantasy' yang serupa.

2.  **Rekomendasi untuk 'Naruto':**
    Anime seperti 'Naruto x UT', 'Naruto: Shippuuden Movie 4 - The Lost Tower', 'Boruto: Naruto the Movie' muncul. Ini sangat relevan
    karena ketiganya adalah anime shounen yang sangat populer dan sering dibandingkan.

Kesimpulan: Model Content-Based Filtering bekerja dengan baik dalam menemukan anime
dengan fitur genre dan tipe yang sangat mirip, menunjukkan relevansi yang tinggi.
Namun, model ini mungkin kurang mampu merekomendasikan anime yang berbeda genre tetapi disukai
oleh pengguna dengan selera yang kompleks (kurang serendipity).
""")

"""## Evaluasi Model Content-Based Filtering (Precision@K dan Recall@K)"""

def calculate_precision_recall_at_k_cbf(anime_name, k, top_recommendations_df, original_df):
    """
    Menghitung Precision@K dan Recall@K untuk Content-Based Filtering.
    Asumsi relevansi: Anime rekomendasi memiliki setidaknya satu genre yang sama
    dengan genre anime input.

    Args:
        anime_name (str): Nama anime input.
        k (int): Jumlah rekomendasi teratas yang dipertimbangkan.
        top_recommendations_df (pd.DataFrame): DataFrame hasil rekomendasi top-K.
        original_df (pd.DataFrame): DataFrame anime asli (anime_cbf) untuk mendapatkan genre.

    Returns:
        tuple: (precision_at_k, recall_at_k)
    """
    if anime_name not in original_df['name'].values:
        return 0.0, 0.0

    # Genre dari anime input
    input_anime_genres_str = original_df[original_df['name'] == anime_name]['genre'].iloc[0]
    input_anime_genres = set(input_anime_genres_str.replace(' ', '').split(',')) # Membersihkan spasi

    if 'UnknownGenre' in input_anime_genres and len(input_anime_genres) == 1:
        print(f"Peringatan: Anime '{anime_name}' hanya memiliki genre 'Unknown Genre'. Relevansi mungkin sulit diukur untuk Precision/Recall.")
        return 0.0, 0.0

    # Identifikasi rekomendasi relevan
    relevant_recommendations = 0
    # Pastikan top_recommendations_df tidak kosong
    if not top_recommendations_df.empty:
        for _, row in top_recommendations_df.head(k).iterrows():
            if 'genre' in row and pd.notna(row['genre']):
                rec_anime_genres = set(str(row['genre']).replace(' ', '').split(',')) # Membersihkan spasi
                # Jika ada setidaknya satu genre yang sama, anggap relevan
                if len(input_anime_genres.intersection(rec_anime_genres)) > 0:
                    relevant_recommendations += 1

    # Precision@K
    precision_at_k = relevant_recommendations / k if k > 0 else 0.0

    # Untuk Recall@K, kita perlu "total item relevan".
    # Ini didefinisikan sebagai semua anime di dataset 'original_df' (kecuali anime input itu sendiri)
    # yang memiliki setidaknya satu genre yang sama dengan anime input.
    total_relevant_items = 0
    for idx, original_row in original_df.iterrows():
        if original_row['name'] == anime_name: # Lewati anime input
            continue
        if pd.notna(original_row['genre']):
            current_anime_genres = set(str(original_row['genre']).replace(' ', '').split(','))
            if len(input_anime_genres.intersection(current_anime_genres)) > 0:
                total_relevant_items += 1

    # Jika total_relevant_items adalah 0, maka recall juga 0 untuk menghindari pembagian dengan nol.
    recall_at_k = relevant_recommendations / total_relevant_items if total_relevant_items > 0 else 0.0

    return precision_at_k, recall_at_k

print("\n--- Evaluasi Kuantitatif Content-Based Filtering (Precision@K & Recall@K) ---")

# Contoh evaluasi untuk 'Fullmetal Alchemist: Brotherhood'
anime_input_cbf_eval = 'Fullmetal Alchemist: Brotherhood'
k_value_cbf_eval = 10
recommended_df_cbf_eval = content_based_recommendations(anime_input_cbf_eval, k_value_cbf_eval)
precision_cbf_val, recall_cbf_val = calculate_precision_recall_at_k_cbf(anime_input_cbf_eval, k_value_cbf_eval, recommended_df_cbf_eval, anime_cbf)
print(f"Untuk '{anime_input_cbf_eval}' dengan K={k_value_cbf_eval}:")
print(f"  Precision@{k_value_cbf_eval}: {precision_cbf_val:.4f}")
print(f"  Recall@{k_value_cbf_eval}: {recall_cbf_val:.4f}")

# Contoh evaluasi untuk 'Naruto'
anime_input_cbf_eval_2 = 'Naruto'
k_value_cbf_eval_2 = 10
recommended_df_cbf_eval_2 = content_based_recommendations(anime_input_cbf_eval_2, k_value_cbf_eval_2)
precision_cbf_val_2, recall_cbf_val_2 = calculate_precision_recall_at_k_cbf(anime_input_cbf_eval_2, k_value_cbf_eval_2, recommended_df_cbf_eval_2, anime_cbf)
print(f"\nUntuk '{anime_input_cbf_eval_2}' dengan K={k_value_cbf_eval_2}:")
print(f"  Precision@{k_value_cbf_eval_2}: {precision_cbf_val_2:.4f}")
print(f"  Recall@{k_value_cbf_eval_2}: {recall_cbf_val_2:.4f}")

"""# Model Development dengan Collaborative Filtering

Collaborative Filtering merekomendasikan item kepada pengguna berdasarkan
kesamaan pola perilaku pengguna lain (user-based) atau kesamaan pola interaksi antar item (item-based).
Di sini, kita akan menggunakan pendekatan Matrix Factorization dengan TensorFlow/Keras
untuk memprediksi rating yang akan diberikan user ke anime.

## Definisikan model Matrix Factorization
"""

class RecommenderNet(keras.Model):
    def __init__(self, num_users, num_anime, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_anime = num_anime
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6) # Regularisasi L2 untuk mencegah overfitting
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.anime_embedding = layers.Embedding(
            num_anime,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6) # Regularisasi L2
        )
        self.anime_bias = layers.Embedding(num_anime, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])  # Embedding user
        user_bias = self.user_bias(inputs[:, 0])         # Bias user
        anime_vector = self.anime_embedding(inputs[:, 1]) # Embedding anime
        anime_bias = self.anime_bias(inputs[:, 1])       # Bias anime

        dot_product = tf.tensordot(user_vector, anime_vector, 2)

        # Tambahkan bias user dan anime
        x = dot_product + user_bias + anime_bias
        # Aktivasi sigmoid untuk mengkompres output ke rentang 0-1
        # Ini penting jika rating dinormalisasi ke 0-1.
        return tf.nn.sigmoid(x)

"""## Inisialisasi model"""

embedding_size = 50
model = RecommenderNet(num_users, num_anime, embedding_size)

"""## Compile model"""

model.compile(
    loss=tf.keras.losses.MeanSquaredError(), # Menggunakan MSE karena ini adalah masalah regresi (prediksi rating)
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()] # Menggunakan RMSE sebagai metrik evaluasi
)

print("\n--- Ringkasan Model Matrix Factorization dengan Keras ---")
model.build(input_shape=(None, 2)) # Membangun model untuk melihat ringkasan
model.summary()

"""## Proses Training Model"""

print("\n--- Proses Training Model Collaborative Filtering (Matrix Factorization) ---")
history = model.fit(
    x=X_train,
    y=y_train,
    batch_size=64,
    epochs=10, # Jumlah epoch bisa disesuaikan
    validation_data=(X_test, y_test),
    verbose=1
)

"""## Plotting hasil training"""

plt.figure(figsize=(12, 6))
plt.plot(history.history['root_mean_squared_error'], label='Train RMSE')
plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')
plt.title('RMSE selama Training dan Validasi')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()
plt.grid(True)
plt.show()

"""## Evaluasi Model

Evaluasi model Collaborative Filtering dengan TensorFlow dilakukan dengan
mengukur seberapa akurat model memprediksi rating pada data testing.
Metrik utama yang digunakan adalah RMSE (Root Mean Squared Error).
"""

print("\n--- Evaluasi Model Collaborative Filtering ---")
loss, rmse = model.evaluate(X_test, y_test, verbose=0)
print(f"Loss pada data test: {loss:.4f}")
print(f"RMSE pada data test: {rmse:.4f}")

# RMSE yang diperoleh adalah dalam skala rating yang dinormalisasi (0-1).
# Untuk menginterpretasikan RMSE dalam skala rating asli (1-10), kita perlu mengalikannya kembali
# dengan rentang rating asli (max_rating - min_rating).
original_rmse = rmse * (max_rating - min_rating)
print(f"RMSE pada skala rating asli (1-10): {original_rmse:.4f}")

# Fungsi Rekomendasi Setelah Model Dilatih
def recommend_for_user(user_id, num_recommendations=10):
    """
    Memberikan rekomendasi anime untuk user tertentu menggunakan model Matrix Factorization.

    Args:
        user_id (int): ID pengguna.
        num_recommendations (int): Jumlah rekomendasi yang diinginkan.

    Returns:
        pd.DataFrame: DataFrame berisi nama anime yang direkomendasikan beserta rating prediksi.
    """
    # Pastikan user_id ada dalam mapper
    if user_id not in user_to_idx:
        print(f"User ID {user_id} tidak ditemukan dalam dataset yang difilter.")
        return pd.DataFrame()

    user_encoded = user_to_idx[user_id]

    # Dapatkan semua anime yang belum pernah dinilai oleh user ini
    rated_anime_ids = merged_data_cf_tf[merged_data_cf_tf['user_id'] == user_id]['anime_id'].tolist()
    all_anime_in_model_ids = list(anime_to_idx.keys()) # Semua anime_id yang ada di model

    unrated_anime_ids = [anime_id for anime_id in all_anime_in_model_ids if anime_id not in rated_anime_ids]

    if not unrated_anime_ids:
        print(f"User ID {user_id} telah menilai semua anime yang tersedia di model.")
        return pd.DataFrame()

    # Buat input untuk prediksi
    user_input = np.array([user_encoded] * len(unrated_anime_ids))
    anime_input = np.array([anime_to_idx[aid] for aid in unrated_anime_ids])

    # Stack user_input and anime_input into a single array
    model_input = np.stack([user_input, anime_input], axis=1)

    # Pass the single stacked array to model.predict()
    predictions = model.predict(model_input).flatten()

    # Ubah prediksi kembali ke skala rating asli
    predictions_original_scale = predictions * (max_rating - min_rating) + min_rating

    # Gabungkan prediksi dengan informasi anime
    recommended_anime_list = []
    for i, anime_id in enumerate(unrated_anime_ids):
        # Dapatkan informasi anime dari dataset 'anime' asli
        info = anime[anime['anime_id'] == anime_id].iloc[0]
        recommended_anime_list.append({
            'anime_id': anime_id,
            'name': info['name'],
            'genre': info['genre'],
            'type': info['type'],
            'predicted_rating': predictions_original_scale[i]
        })

    recommended_df = pd.DataFrame(recommended_anime_list)
    recommended_df = recommended_df.sort_values(by='predicted_rating', ascending=False)
    return recommended_df.head(num_recommendations)

print("\n--- Demonstrasi Rekomendasi Collaborative Filtering (TensorFlow) ---")
# Pilih user_id dari user yang ada di merged_data_cf_tf
example_user_id = merged_data_cf_tf['user_id'].iloc[0] # Ambil user_id pertama dari data yang sudah difilter
print(f"Rekomendasi untuk User ID: {example_user_id}")
recommended_anime_tf = recommend_for_user(example_user_id, 10)
print(recommended_anime_tf)

print(f"\nRekomendasi untuk User ID: {merged_data_cf_tf['user_id'].iloc[150]}")
recommended_anime_tf_2 = recommend_for_user(merged_data_cf_tf['user_id'].iloc[150], 10)
print(recommended_anime_tf_2)

"""# Perbandingan dan Pemilihan Solusi

Memilih antara Content-Based Filtering dan Collaborative Filtering tergantung pada
karakteristik data dan tujuan rekomendasi.

**Content-Based Filtering:**
-   **Cocok untuk:** Cold-start problem bagi pengguna baru (karena tidak butuh riwayat interaksi).
    Mampu merekomendasikan item yang belum pernah diberi rating.
-   **Kelebihan:** Penjelasan rekomendasi mudah, tidak ada masalah cold-start user.
-   **Kekurangan:** Kurang eksploratif, hanya merekomendasikan item yang mirip, butuh fitur item yang kaya.

**Collaborative Filtering (dengan TensorFlow/Matrix Factorization):**
-   **Cocok untuk:** Menemukan pola tersembunyi dalam interaksi pengguna. Mampu merekomendasikan item yang tidak mirip dengan yang sudah dikenal user.
-   **Kelebihan:** Lebih eksploratif (serendipity), tidak butuh fitur item eksplisit (cukup ID user dan item), dapat menangani jumlah data yang besar dengan efisien.
-   **Kekurangan:** Masalah cold-start user dan item (membutuhkan riwayat interaksi yang cukup), sparsity data dapat menjadi tantangan.

**Solusi Permasalahan:**
Untuk sistem rekomendasi anime, kombinasi kedua pendekatan (Hybrid Recommender System) seringkali merupakan solusi terbaik untuk mengatasi kelemahan masing-masing.

-   **Jika fokus adalah merekomendasikan anime berdasarkan genre atau deskripsi:** Content-Based Filtering sangat relevan.
-   **Jika fokus adalah menemukan anime yang disukai oleh "orang seperti Anda" atau anime yang "mirip" berdasarkan pola tontonan:** Collaborative Filtering lebih tepat.

Dengan implementasi Matrix Factorization menggunakan TensorFlow, kita mendapatkan model Collaborative Filtering
yang scalable dan dapat belajar representasi (embedding) user dan anime secara otomatis dari data interaksi.
Ini adalah pendekatan yang kuat untuk sistem rekomendasi modern.


**Kesimpulan Akhir:**
Mengingat tujuan proyek untuk membuat sistem rekomendasi, kedua pendekatan memiliki keunggulan masing-masing.
Untuk sistem yang robust dan komprehensif, disarankan untuk mengimplementasikan **Hybrid Recommender System**
yang menggabungkan kekuatan Content-Based Filtering (untuk mengatasi cold-start item dan transparansi rekomendasi)
dan Collaborative Filtering (untuk serendipity dan menangkap pola kompleks interaksi).

Namun, jika harus memilih satu pendekatan untuk efektivitas dan skalabilitas pada data interaksi yang besar,
**Collaborative Filtering dengan Matrix Factorization (TensorFlow)** adalah pilihan yang sangat kuat.
Ini mampu mempelajari preferensi tersembunyi dan menghasilkan rekomendasi yang personal.
"""